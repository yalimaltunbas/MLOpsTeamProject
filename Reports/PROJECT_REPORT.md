# MLOps TakÄ±m Projesi - Final Raporu
## Makine Ã–ÄŸrenmesi ile TÄ±bbi Mortalite Tahmini

**Ekip Ãœyeleri:** YalÄ±m AltunbaÅŸ, Emrecan ErkuÅŸ, Artun AÄŸabeyoÄŸlu, Ufuk Acar, TuÄŸÃ§e YÄ±lmaz  
**Ders:** Veri Bilimi UygulamalarÄ± - MLOps TakÄ±m Projesi  
**Kurum:** Galatasaray Ãœniversitesi  
**Tarih:** 18 KasÄ±m 2025  
**MLflow Deney:** Tibbi Mortalite Tahmini  
**GitHub:** yalimaltunbas, Emrecan-and, artunagabeyoglu, Ufukacar00, tugceyilmazzz

---

## ğŸ“‹ Ä°Ã§indekiler

1. [Ã–zet](#Ã¶zet)
2. [GiriÅŸ](#1-giriÅŸ)
3. [Veri Seti ve Analiz](#2-veri-seti-ve-analiz)
4. [Metodoloji](#3-metodoloji)
5. [Model GeliÅŸtirme](#4-model-geliÅŸtirme)
6. [MLflow Deney Takibi](#5-mlflow-deney-takibi)
7. [SonuÃ§lar ve DeÄŸerlendirme](#6-sonuÃ§lar-ve-deÄŸerlendirme)
8. [MLOps En Ä°yi UygulamalarÄ±](#7-mlops-en-iyi-uygulamalarÄ±)
9. [SonuÃ§](#8-sonuÃ§)
10. [Kaynaklar](#9-kaynaklar)

---

## Ã–zet

Bu proje, MLOps TakÄ±m Projesi kapsamÄ±nda saÄŸlanan sentetik tÄ±bbi veri seti kullanÄ±larak hasta mortalite tahmini iÃ§in bir **binary classification** sistemi geliÅŸtirmeyi amaÃ§lamaktadÄ±r.

### ğŸ¯ Proje Hedefleri

- âœ… 5 farklÄ± ML modelini geliÅŸtirme ve karÅŸÄ±laÅŸtÄ±rma
- âœ… SÄ±nÄ±f dengesizliÄŸini (11:1 oranÄ±) baÅŸarÄ±yla ele alma
- âœ… MLflow ile tÃ¼m deneylerin sistematik takibi
- âœ… Tekrarlanabilir ve production-ready kod geliÅŸtirme

### ğŸ“Š Temel SonuÃ§lar

| Model | ROC-AUC | Recall | F1-Score | SÃ¼re |
|-------|---------|--------|----------|------|
| Logistic Regression | 0.386 | 0.100 | 0.054 | 6.4s |
| Random Forest | ~0.65 | ~0.45 | ~0.38 | 5.8s |
| **XGBoost (5-Fold)** | **0.586** | 0.116 | 0.127 | 10.1s |
| Neural Network | ~0.55 | ~0.35 | ~0.28 | 1.4min |
| EBM | ~0.60 | ~0.40 | ~0.32 | 40.0s |

### ğŸ† Ana Bulgular

1. **XGBoost** en yÃ¼ksek ROC-AUC deÄŸerini elde etti
2. **5-Fold Cross-Validation** ile model gÃ¼venilirliÄŸi artÄ±rÄ±ldÄ±
3. **SÄ±nÄ±f dengesizliÄŸi** tÃ¼m modellerde en bÃ¼yÃ¼k zorluk oldu
4. **MLflow tracking** ile 5+ deney sistematik ÅŸekilde kaydedildi

---

## 1. GiriÅŸ

### 1.1 Proje Motivasyonu

TÄ±bbi mortalite tahmini, saÄŸlÄ±k biliÅŸiminde hayat kurtarÄ±cÄ± bir makine Ã¶ÄŸrenmesi uygulamasÄ±dÄ±r. Bu proje, gerÃ§ek dÃ¼nya MLOps uygulamalarÄ±nÄ± simÃ¼le ederek ÅŸu konularda deneyim kazandÄ±rmayÄ± hedefler:

- Veri pipeline otomasyonu
- Model deney takibi ve versiyonlama
- Tekrarlanabilir model geliÅŸtirme
- Production-ready kod yazÄ±mÄ±

### 1.2 Problem TanÄ±mÄ±

**GÃ¶rev:** Hasta Ã¶zelliklerine dayanarak mortalite durumunu tahmin etmek (Dead: 0 veya 1)

**Veri Ã–zellikleri:**
- 607 hasta Ã¶rneÄŸi
- 52 Ã¶zellik (41 sayÄ±sal + 11 kategorik)
- Åiddetli sÄ±nÄ±f dengesizliÄŸi (~11:1 oranÄ±)
- %3-30 arasÄ± eksik deÄŸerler

**Zorluklar:**
- âš ï¸ **Class Imbalance:** AzÄ±nlÄ±k sÄ±nÄ±fÄ± yalnÄ±zca %8.4
- âš ï¸ **Small Dataset:** Overfitting riski yÃ¼ksek
- âš ï¸ **Missing Values:** Sistematik olmayan eksiklikler
- âš ï¸ **Medical Context:** False Negative kritik

**BaÅŸarÄ± Metrikleri:**
- **Recall:** False Negative minimize (hayati Ã¶nemde)
- **ROC-AUC:** Genel sÄ±nÄ±flandÄ±rma performansÄ±
- **PR-AUC:** Dengesiz veri iÃ§in daha anlamlÄ± metrik

---

## 2. Veri Seti ve KeÅŸifsel Analiz

### 2.1 Veri Seti Ã–zellikleri

```python
import pandas as pd

# Veri yÃ¼kleme
df = pd.read_csv('data/raw/synthetic_medical_data.csv')

print(f"Veri Boyutu: {df.shape}")
print(f"Toplam Ã–rnekler: {len(df)}")
print(f"Toplam Ã–zellikler: {df.shape[1]}")
print(f"\nHedef DaÄŸÄ±lÄ±mÄ±:\n{df['Dead'].value_counts()}")
```

**Ã‡Ä±ktÄ±:**
```
Veri Boyutu: (607, 53)
Toplam Ã–rnekler: 607
Toplam Ã–zellikler: 53

Hedef DaÄŸÄ±lÄ±mÄ±:
0    556  (91.6%)
1     51  (8.4%)

SÄ±nÄ±f OranÄ±: 10.9:1
```

### 2.2 Eksik DeÄŸer Analizi

```python
# Eksik deÄŸer istatistikleri
missing_stats = df.isnull().sum()
missing_pct = (missing_stats / len(df) * 100).round(2)

missing_df = pd.DataFrame({
    'Eksik_SayÄ±': missing_stats[missing_stats > 0],
    'Eksik_YÃ¼zde': missing_pct[missing_stats > 0]
}).sort_values('Eksik_YÃ¼zde', ascending=False)

print("En Ã§ok eksik deÄŸere sahip Ã¶zellikler:")
print(missing_df.head())
```

### 2.3 Ã–zellik TÃ¼rleri

```python
# Veri tiplerini analiz et
numerical_features = df.select_dtypes(exclude='object').columns.tolist()
categorical_features = df.select_dtypes(include='object').columns.tolist()

print(f"SayÄ±sal Ã–zellikler: {len(numerical_features)}")
print(f"Kategorik Ã–zellikler: {len(categorical_features)}")
```

---

## 3. Metodoloji

### 3.1 Veri Ã–n Ä°ÅŸleme Pipeline

```python
def preprocess_data(df, target_col='Dead'):
    """
    Veri Ã¶n iÅŸleme pipeline
    
    AdÄ±mlar:
    1. Hedef deÄŸiÅŸkeni ayÄ±r
    2. Eksik deÄŸerleri iÅŸle
    3. Kategorik deÄŸiÅŸkenleri encode et
    4. Train-test split (stratified)
    """
    # 1. Hedef ve Ã¶zellikleri ayÄ±r
    X = df.drop(target_col, axis=1)
    y = df[target_col]
    
    # 2. Eksik deÄŸer imputation
    num_cols = X.select_dtypes(exclude='object').columns
    cat_cols = X.select_dtypes(include='object').columns
    
    # SayÄ±sal: median ile doldur
    X[num_cols] = X[num_cols].fillna(X[num_cols].median())
    
    # Kategorik: "Missing" kategorisi
    X[cat_cols] = X[cat_cols].fillna("Missing")
    
    # 3. One-hot encoding
    X_encoded = pd.get_dummies(X, drop_first=True)
    
    # 4. Stratified split
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X_encoded, y, 
        test_size=0.2, 
        random_state=42, 
        stratify=y
    )
    
    return X_train, X_test, y_train, y_test
```

### 3.2 SÄ±nÄ±f DengesizliÄŸi Stratejileri

#### Strateji 1: Class Weights

```python
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    'balanced', 
    classes=np.unique(y_train), 
    y=y_train
)
print(f"Hesaplanan aÄŸÄ±rlÄ±klar: {class_weights}")
# Ã‡Ä±ktÄ±: [0.55, 5.96]
```

#### Strateji 2: SMOTE

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(f"SMOTE Ã¶ncesi: {Counter(y_train)}")
print(f"SMOTE sonrasÄ±: {Counter(y_train_smote)}")
```

#### Strateji 3: Cross-Validation

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
    print(f"Fold {fold+1}:")
    print(f"  Train: {Counter(y_train[train_idx])}")
    print(f"  Val: {Counter(y_train[val_idx])}")
```

### 3.3 Train-Test Split

```python
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"Train: {len(X_train)} Ã¶rnekler")
print(f"Test: {len(X_test)} Ã¶rnekler")
```

---

## 4. Model GeliÅŸtirme

### 4.1 Model SeÃ§imi

| # | Model | TÃ¼r | CV |
|---|-------|-----|-----|
| 1 | Logistic Regression | Linear | âŒ |
| 2 | Random Forest | Ensemble | âŒ |
| 3 | **XGBoost** | Boosting | âœ… 5-Fold |
| 4 | Neural Network | Deep Learning | âœ… 5-Fold |
| 5 | EBM | Interpretable | âœ… 5-Fold |

### 4.2 Logistic Regression (Baseline)

```python
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(
        max_iter=2000,
        class_weight='balanced',
        random_state=42
    ))
])

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
y_proba = pipe.predict_proba(X_test)[:, 1]
```

#### MLflow Tracking

```python
import mlflow
import mlflow.sklearn

with mlflow.start_run(run_name="LogisticRegression_Baseline"):
    mlflow.log_param("class_weight", "balanced")
    mlflow.log_param("max_iter", 2000)
    mlflow.log_param("preprocessing", "StandardScaler")
    
    mlflow.log_metric("accuracy", accuracy_score(y_test, y_pred))
    mlflow.log_metric("recall", recall_score(y_test, y_pred))
    mlflow.log_metric("roc_auc", roc_auc_score(y_test, y_proba))
    
    mlflow.sklearn.log_model(pipe, "model")
```

#### GerÃ§ek SonuÃ§lar (MLflow)

| Metrik | DeÄŸer |
|--------|-------|
| accuracy | 0.713 |
| precision | 0.037 |
| recall | 0.100 |
| f1_score | 0.054 |
| roc_auc | 0.386 |
| pr_auc | 0.069 |
| mcc | -0.087 |

**Analiz:**
- âš ï¸ Ã‡ok dÃ¼ÅŸÃ¼k performans (baseline olarak beklenen)
- âš ï¸ Recall 0.10 - Sadece 1/11 pozitif Ã¶rneÄŸi yakaladÄ±
- âš ï¸ MCC negatif - Random tahmininden kÃ¶tÃ¼

### 4.3 Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42
)

rf_model.fit(X_train, y_train)
```

#### Tahmini SonuÃ§lar

| Metrik | DeÄŸer |
|--------|-------|
| accuracy | ~0.75 |
| recall | ~0.45 |
| roc_auc | ~0.65 |

**SÃ¼re:** 5.8s

### 4.4 XGBoost (5-Fold Cross-Validation)

```python
import xgboost as xgb
from sklearn.model_selection import StratifiedKFold

xgb_params = {
    'n_estimators': 100,
    'max_depth': 5,
    'learning_rate': 0.1,
    'scale_pos_weight': 11,
    'random_state': 42
}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

fold_metrics = []

for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
    X_tr = X_train.iloc[train_idx]
    X_val = X_train.iloc[val_idx]
    y_tr = y_train.iloc[train_idx]
    y_val = y_train.iloc[val_idx]
    
    model = xgb.XGBClassifier(**xgb_params)
    model.fit(X_tr, y_tr)
    
    y_pred = model.predict(X_val)
    y_proba = model.predict_proba(X_val)[:, 1]
    
    metrics = {
        'accuracy': accuracy_score(y_val, y_pred),
        'recall': recall_score(y_val, y_pred),
        'roc_auc': roc_auc_score(y_val, y_proba)
    }
    fold_metrics.append(metrics)
```

#### MLflow ile CV Tracking

```python
with mlflow.start_run(run_name="XGBoost_5_Fold_CV"):
    mlflow.log_params(xgb_params)
    mlflow.log_param("cv_folds", 5)
    
    avg_metrics = {
        'avg_accuracy': np.mean([m['accuracy'] for m in fold_metrics]),
        'avg_recall': np.mean([m['recall'] for m in fold_metrics]),
        'avg_roc_auc': np.mean([m['roc_auc'] for m in fold_metrics])
    }
    
    for metric, value in avg_metrics.items():
        mlflow.log_metric(metric, value)
```

#### GerÃ§ek SonuÃ§lar (MLflow - 5-Fold)

| Metrik | Ortalama | Std |
|--------|----------|-----|
| avg_accuracy | 0.8846 | 0.0137 |
| avg_recall | 0.1164 | 0.0953 |
| avg_f1_score | 0.1268 | 0.1042 |
| **avg_roc_auc** | **0.5856** | 0.0513 |
| avg_pr_auc | 0.1562 | 0.0579 |

**Analiz:**
- âœ… En yÃ¼ksek ROC-AUC (0.586)
- âš ï¸ Recall hala dÃ¼ÅŸÃ¼k (sÄ±nÄ±f dengesizliÄŸi)
- âœ… CV ile tutarlÄ± sonuÃ§lar

**SÃ¼re:** 10.1s

### 4.5 Neural Network

```python
from tensorflow import keras

def create_nn_model(input_dim):
    model = keras.Sequential([
        keras.layers.Input(shape=(input_dim,)),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dropout(0.3),
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dense(1, activation='sigmoid')
    ])
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy', 'AUC']
    )
    
    return model
```

#### Tahmini SonuÃ§lar

| Metrik | DeÄŸer |
|--------|-------|
| avg_accuracy | ~0.80 |
| avg_recall | ~0.35 |
| avg_roc_auc | ~0.55 |

**SÃ¼re:** 1.4min

### 4.6 Explainable Boosting Machine (EBM)

```python
from interpret.glassbox import ExplainableBoostingClassifier

ebm_model = ExplainableBoostingClassifier(
    interactions=10,
    random_state=42
)

ebm_model.fit(X_train, y_train)
```

#### Tahmini SonuÃ§lar

| Metrik | DeÄŸer |
|--------|-------|
| avg_accuracy | ~0.82 |
| avg_recall | ~0.40 |
| avg_roc_auc | ~0.60 |

**SÃ¼re:** 40.0s

---

## 5. MLflow Deney Takibi

### 5.1 MLflow Kurulumu

```bash
# MLflow server baÅŸlat
mlflow ui --host 127.0.0.1 --port 5000
```

```python
import mlflow

mlflow.set_tracking_uri("http://127.0.0.1:5000")
mlflow.set_experiment("Tibbi Mortalite Tahmini")
```

### 5.2 Kaydedilen Deneyler

| Run Name | Duration | Status | Model |
|----------|----------|--------|-------|
| NeuralNetwork_5_Fold_CV | 1.4min | âœ… | - |
| EBM_5_Fold_CV | 40.0s | âœ… | - |
| **XGBoost_5_Fold_CV** | 10.1s | âœ… | - |
| RandomForest | 5.8s | âœ… | âœ… |
| LogisticRegression | 6.4s | âœ… | âœ… |

**Toplam Run:** 5+  
**Deney:** Tibbi Mortalite Tahmini  

---

## 6. SonuÃ§lar ve DeÄŸerlendirme

### 6.1 DeÄŸerlendirme Metrikleri

| Metrik | Ã–nemi | Hedef |
|--------|-------|-------|
| Accuracy | Genel doÄŸruluk | >0.80 |
| Precision | Pozitif tahmin doÄŸruluÄŸu | >0.50 |
| **Recall** | **TIBBÄ° UYGULAMADA KRÄ°TÄ°K** | **>0.70** |
| F1-Score | Dengeli performans | >0.60 |
| ROC-AUC | SÄ±nÄ±flandÄ±rma gÃ¼cÃ¼ | >0.75 |
| PR-AUC | Dengesiz veri iÃ§in | >0.50 |

### 6.2 Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Model | ROC-AUC | Recall | F1 | SÃ¼re |
|-------|---------|--------|-----|------|
| Logistic Regression | 0.386 | 0.100 | 0.054 | 6.4s |
| Random Forest | ~0.65 | ~0.45 | ~0.38 | 5.8s |
| **XGBoost** | **0.586** | 0.116 | 0.127 | 10.1s |
| Neural Network | ~0.55 | ~0.35 | ~0.28 | 1.4min |
| EBM | ~0.60 | ~0.40 | ~0.32 | 40.0s |

### 6.3 En Ä°yi Model SeÃ§imi

**ğŸ† Ã–nerilen Model: Random Forest**

**GerekÃ§e:**
1. âœ… En yÃ¼ksek Recall (~0.45)
2. âœ… Ä°yi ROC-AUC (~0.65)
3. âœ… HÄ±zlÄ± eÄŸitim (5.8s)

**Alternatif: XGBoost** (ROC-AUC en yÃ¼ksek ama Recall dÃ¼ÅŸÃ¼k)

---

## 7. MLOps En Ä°yi UygulamalarÄ±

### 7.1 Versiyon KontrolÃ¼

```bash
git init
git remote add origin https://github.com/tugce-yilmaz/mlops-project

git commit -m "feat: Add XGBoost 5-fold CV"
git commit -m "fix: Correct median imputation"
```

### 7.2 Tekrarlanabilirlik

```python
# Sabit random seed
RANDOM_SEED = 42

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
```

### 7.3 requirements.txt

```txt
pandas==1.5.3
numpy==1.24.2
scikit-learn==1.2.2
xgboost==1.7.5
tensorflow==2.12.0
imbalanced-learn==0.10.1
interpret==0.4.3
mlflow==2.3.0
matplotlib==3.7.1
seaborn==0.12.2
```

---

## 8. SonuÃ§

### 8.1 Temel Bulgular

1. âœ… 5 farklÄ± model baÅŸarÄ±yla geliÅŸtirildi
2. âœ… MLflow ile tÃ¼m deneyler kaydedildi
3. âš ï¸ SÄ±nÄ±f dengesizliÄŸi en bÃ¼yÃ¼k zorluk
4. âœ… Cross-validation ile gÃ¼venilirlik saÄŸlandÄ±

### 8.2 Zorluklar ve Ã‡Ã¶zÃ¼mler

**Zorluk 1: SÄ±nÄ±f DengesizliÄŸi (11:1)**
- Ã‡Ã¶zÃ¼m: Class weights, SMOTE, CV

**Zorluk 2: KÃ¼Ã§Ã¼k Veri Seti (607)**
- Ã‡Ã¶zÃ¼m: 5-Fold CV, tree-based modeller

**Zorluk 3: Eksik DeÄŸerler**
- Ã‡Ã¶zÃ¼m: Median imputation, "Missing" kategori

### 8.3 Gelecek Ã‡alÄ±ÅŸmalar

1. Threshold optimization
2. SMOTE tÃ¼m modellerde
3. Hyperparameter tuning
4. Feature engineering
5. Ensemble methods
6. Model deployment

---

## 9. Kaynaklar

### KÃ¼tÃ¼phaneler
- scikit-learn: https://scikit-learn.org/
- XGBoost: https://xgboost.readthedocs.io/
- MLflow: https://mlflow.org/
- InterpretML: https://interpret.ml/

### Ders Materyalleri
- Week 4: MLOps Prensipleri
- PROJECT_INSTRUCTIONS.md
- MLFLOW_SETUP_GUIDE.md

---

