# MLOps TakÄ±m Projesi - Final Raporu
## Makine Ã–ÄŸrenmesi ile TÄ±bbi Mortalite Tahmini

**Ekip Ãœyesi:** TuÄŸÃ§e YÄ±lmaz  
**Ders:** Veri Bilimi UygulamalarÄ± - MLOps TakÄ±m Projesi  
**Kurum:** Galatasaray Ãœniversitesi  
**Tarih:** 17 Ocak 2025  
**MLflow Deney:** Team_TugceYilmaz_Experiments  
**GitHub:** tugce-yilmaz_tpkapi

---

## ğŸ“‹ Ä°Ã§indekiler

1. [YÃ¶netici Ã–zeti](#yÃ¶netici-Ã¶zeti)
2. [GiriÅŸ](#1-giriÅŸ)
   - 1.1 [Proje Motivasyonu](#11-proje-motivasyonu)
   - 1.2 [Problem TanÄ±mÄ±](#12-problem-tanÄ±mÄ±)
   - 1.3 [Veri Setine Genel BakÄ±ÅŸ](#13-veri-setine-genel-bakÄ±ÅŸ)
3. [Veri Anlama ve KeÅŸif](#2-veri-anlama-ve-keÅŸif)
   - 2.1 [Veri Seti Ã–zellikleri](#21-veri-seti-Ã¶zellikleri)
   - 2.2 [KeÅŸifsel Veri Analizi (EDA)](#22-keÅŸifsel-veri-analizi-eda)
   - 2.3 [Veri Kalitesi SorunlarÄ±](#23-veri-kalitesi-sorunlarÄ±)
4. [Metodoloji](#3-metodoloji)
   - 3.1 [Veri Ã–n Ä°ÅŸleme Pipeline](#31-veri-Ã¶n-iÌ‡ÅŸleme-pipeline)
   - 3.2 [Ã–zellik MÃ¼hendisliÄŸi](#32-Ã¶zellik-mÃ¼hendisliÄŸi)
   - 3.3 [SÄ±nÄ±f DengesizliÄŸi ile BaÅŸa Ã‡Ä±kma](#33-sÄ±nÄ±f-dengesizliÄŸi-ile-baÅŸa-Ã§Ä±kma)
   - 3.4 [Train-Test AyrÄ±mÄ± Stratejisi](#34-train-test-ayrÄ±mÄ±-stratejisi)
5. [Model GeliÅŸtirme](#4-model-geliÅŸtirme)
   - 4.1 [Model SeÃ§imi](#41-model-seÃ§imi)
   - 4.2 [Logistic Regression (Baseline)](#42-logistic-regression-baseline)
   - 4.3 [Random Forest](#43-random-forest)
   - 4.4 [XGBoost](#44-xgboost)
   - 4.5 [Yapay Sinir AÄŸÄ± (Neural Network)](#45-yapay-sinir-aÄŸÄ±-neural-network)
   - 4.6 [Explainable Boosting Machine (EBM)](#46-explainable-boosting-machine-ebm)
6. [MLflow Deney Takibi](#5-mlflow-deney-takibi)
   - 5.1 [Deney Kurulumu](#51-deney-kurulumu)
   - 5.2 [Kaydedilen Metrikler ve Parametreler](#52-kaydedilen-metrikler-ve-parametreler)
   - 5.3 [Model Versiyonlama](#53-model-versiyonlama)
7. [SonuÃ§lar ve DeÄŸerlendirme](#6-sonuÃ§lar-ve-deÄŸerlendirme)
   - 6.1 [DeÄŸerlendirme Metrikleri](#61-deÄŸerlendirme-metrikleri)
   - 6.2 [Model KarÅŸÄ±laÅŸtÄ±rmasÄ±](#62-model-karÅŸÄ±laÅŸtÄ±rmasÄ±)
   - 6.3 [En Ä°yi Model SeÃ§imi](#63-en-iÌ‡yi-model-seÃ§imi)
8. [MLOps En Ä°yi UygulamalarÄ±](#7-mlops-en-iÌ‡yi-uygulamalarÄ±)
   - 7.1 [Versiyon KontrolÃ¼](#71-versiyon-kontrolÃ¼)
   - 7.2 [Tekrarlanabilirlik](#72-tekrarlanabilirlik)
   - 7.3 [Kod Kalitesi](#73-kod-kalitesi)
9. [SonuÃ§](#8-sonuÃ§)
   - 8.1 [Temel Bulgular](#81-temel-bulgular)
   - 8.2 [Zorluklar ve Ã‡Ã¶zÃ¼mler](#82-zorluklar-ve-Ã§Ã¶zÃ¼mler)
   - 8.3 [Gelecek Ã‡alÄ±ÅŸmalar](#83-gelecek-Ã§alÄ±ÅŸmalar)
10. [Kaynaklar](#9-kaynaklar)
11. [Ekler](#10-ekler)

---

## YÃ¶netici Ã–zeti

Bu proje, MLOps TakÄ±m Projesi kapsamÄ±nda saÄŸlanan sentetik tÄ±bbi veri seti kullanÄ±larak tÄ±bbi mortalite tahmini iÃ§in bir **binary classification (ikili sÄ±nÄ±flandÄ±rma)** problemini ele almaktadÄ±r. Birincil hedef, MLflow ile deney takibi, Git ile versiyon kontrolÃ¼ ve tekrarlanabilir pipeline'lar dahil olmak Ã¼zere **MLOps en iyi uygulamalarÄ±nÄ±** takip ederek **beÅŸ farklÄ± makine Ã¶ÄŸrenmesi modelini** geliÅŸtirmek, deÄŸerlendirmek ve karÅŸÄ±laÅŸtÄ±rmaktÄ±r.

### ğŸ¯ Proje Hedefleri
- âœ… 5 gerekli ML modelini geliÅŸtirme ve karÅŸÄ±laÅŸtÄ±rma
- âœ… SÄ±nÄ±f dengesizliÄŸini (11:1 oranÄ±) ele alma
- âœ… KapsamlÄ± veri Ã¶n iÅŸleme uygulama
- âœ… MLflow kullanarak 50+ deney takibi
- âœ… Tekrarlanabilirlik iÃ§in MLOps prensiplerine uyma

### ğŸ“Š Temel SonuÃ§lar
- **En Ä°yi Model:** XGBoost 
- **Toplam MLflow Run:** 50+ deney kaydedildi
- **SÄ±nÄ±f DengesizliÄŸi Ã‡Ã¶zÃ¼mÃ¼:** SMOTE + threshold ayarlamasÄ±
- **Deployment HazÄ±rlÄ±ÄŸÄ±:** En iyi model MLflow Model Registry'ye kaydedildi

### ğŸ† Ana Bulgular
1. **XGBoost** tÃ¼m metriklerde en yÃ¼ksek performansÄ± gÃ¶sterdi
2. **SMOTE** Random Forest performansÄ±nÄ± Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rdÄ±
3. **Threshold optimizasyonu** Recall'u maksimize etmek iÃ§in kritikti
4. **EBM** klinik uygulamalar iÃ§in mÃ¼kemmel yorumlanabilirlik saÄŸladÄ±
5. **Neural Network** umut verici ancak daha fazla veriye ihtiyaÃ§ var

---

## 1. GiriÅŸ

### 1.1 Proje Motivasyonu

TÄ±bbi mortalite tahmini, saÄŸlÄ±k alanÄ±nda makine Ã¶ÄŸrenmesinin kritik bir uygulamasÄ±dÄ±r. Bu proje, **sÄ±nÄ±f dengesizliÄŸi**, **eksik deÄŸerler** ve **yorumlanabilirlik gereksinimlerinin** Ã¶nemli zorluklar oluÅŸturduÄŸu gerÃ§ek dÃ¼nya senaryosunu simÃ¼le eder.

Proje **MLOps yaÅŸam dÃ¶ngÃ¼sÃ¼nÃ¼** takip eder:
1. Veri versiyonlama ve Ã¶n iÅŸleme
2. Deney takibi ve model karÅŸÄ±laÅŸtÄ±rmasÄ±
3. Model registry ve deployment hazÄ±rlÄ±ÄŸÄ±
4. Otomasyon ile tekrarlanabilirlik

### 1.2 Problem TanÄ±mÄ±

**GÃ¶rev:** Hasta mortalitesini tahmin etmek iÃ§in binary classification (Dead: 0 veya 1)

**Zorluklar:**
- âš ï¸ **SÄ±nÄ±f DengesizliÄŸi:** 556 hayatta vs 51 vefat (~11:1 oranÄ±)
- âš ï¸ **Eksik DeÄŸerler:** Ã–zelliklerde %3-30 arasÄ±
- âš ï¸ **KarÄ±ÅŸÄ±k Ã–zellik TÃ¼rleri:** 41 sayÄ±sal + 11 kategorik
- âš ï¸ **Klinik BaÄŸlam:** False Negative'ler False Positive'lerden daha maliyetli

**BaÅŸarÄ± Kriterleri:**
- **Recall**'u maksimize etmek (tÃ¼m mortalite vakalarÄ±nÄ± yakalamak)
- YÃ¼ksek **ROC-AUC** ve **PR-AUC** (dengesizliÄŸi ele almak)
- Model **yorumlanabilirliÄŸi** (EBM, feature importance)

### 1.3 Veri Setine Genel BakÄ±ÅŸ

**Kaynak:** `synthetic_medical_data.csv` (`generate_synthetic_data.py` ile Ã¼retildi)

**Boyutlar:**
- **Ã–rnekler:** 607 hasta
- **Ã–zellikler:** 52 (41 sayÄ±sal + 11 kategorik)
- **Hedef:** Dead (0 = Hayatta, 1 = Vefat)

**Ã–zellik Kategorileri:**
- Demografik: YaÅŸ, Cinsiyet, Etnik kÃ¶ken
- TÄ±bbi GeÃ§miÅŸ: TÃ¼mÃ¶r boyutu, hormon seviyeleri, vb.
- Tedavi: Ä°laÃ§ kullanÄ±mÄ±, tÄ±bbi prosedÃ¼rler
- Laboratuvar SonuÃ§larÄ±: Ã‡eÅŸitli biyobelirteÃ§ler

---

## 2. Veri Anlama ve KeÅŸif

### 2.1 Veri Seti Ã–zellikleri

```python
# Veri seti boyutu
print(f"Toplam Ã¶rnek sayÄ±sÄ±: {len(df)}")
print(f"Toplam Ã¶zellik sayÄ±sÄ±: {df.shape[1]}")
print(f"\nHedef daÄŸÄ±lÄ±mÄ±:\n{df['Dead'].value_counts()}")
```

**Ã‡Ä±ktÄ±:**
```
Toplam Ã¶rnek sayÄ±sÄ±: 607
Toplam Ã¶zellik sayÄ±sÄ±: 53

Hedef daÄŸÄ±lÄ±mÄ±:
0    556
1     51
Name: Dead, dtype: int64
```

**SÄ±nÄ±f DengesizliÄŸi OranÄ±:** 10.9:1 (Ã‡oÄŸunluk:AzÄ±nlÄ±k)

### 2.2 KeÅŸifsel Veri Analizi (EDA)

EDA'dan elde edilen temel bulgular (`notebooks/01_exploratory_data_analysis.ipynb`):

1. **Eksik DeÄŸer DaÄŸÄ±lÄ±mÄ±:**
   - SayÄ±sal Ã¶zellikler: %3-30 eksik
   - Kategorik Ã¶zellikler: %5-25 eksik
   - Sistematik bir pattern tespit edilmedi

2. **Ã–zellik KorelasyonlarÄ±:**
   - BazÄ± Ã¶zellikler yÃ¼ksek korelasyonlu (>0.8)
   - Feature selection performansÄ± artÄ±rabilir

3. **Hedef SÄ±nÄ±f Analizi:**
   - AzÄ±nlÄ±k sÄ±nÄ±fÄ± (Dead=1) ÅŸu Ã¶zelliklerde farklÄ± pattern gÃ¶sterir:
     - YaÅŸ daÄŸÄ±lÄ±mÄ±
     - TÃ¼mÃ¶r boyutu
     - Hormon seviyeleri

### 2.3 Veri Kalitesi SorunlarÄ±

| Sorun | Etkilenen Ã–zellikler | Uygulanan Ã‡Ã¶zÃ¼m |
|-------|---------------------|-----------------|
| Eksik deÄŸerler | 40+ Ã¶zellik | Median imputation (sayÄ±sal), "Missing" kategorisi (kategorik) |
| SÄ±nÄ±f dengesizliÄŸi | Hedef deÄŸiÅŸken | SMOTE, class weights, threshold tuning |
| KarÄ±ÅŸÄ±k veri tipleri | TÃ¼m Ã¶zellikler | SayÄ±sal/kategorik iÃ§in ayrÄ± pipeline'lar |
| Potansiyel outlier'lar | SayÄ±sal Ã¶zellikler | Tree-based modeller (outlier'lara dayanÄ±klÄ±) |

---

## 3. Metodoloji

### 3.1 Veri Ã–n Ä°ÅŸleme Pipeline

Ã–n iÅŸleme **modÃ¼ler, tekrarlanabilir bir pipeline** takip eder (`src/data_preprocessing.py`):

```python
def preprocess_data(df, target_col='Dead'):
    """
    Tam Ã¶n iÅŸleme pipeline
    
    AdÄ±mlar:
    1. Ã–zellikleri ve hedefi ayÄ±r
    2. Eksik deÄŸerleri iÅŸle
    3. Kategorik deÄŸiÅŸkenleri encode et
    4. Stratified train-test split
    
    Returns:
        X_train, X_test, y_train, y_test
    """
    # AdÄ±m 1: Hedefi ayÄ±r
    X = df.drop(target_col, axis=1)
    y = df[target_col]
    
    # AdÄ±m 2: Eksik deÄŸer imputation
    X_processed = handle_missing_values(X)
    
    # AdÄ±m 3: Encoding
    X_encoded = encode_features(X_processed)
    
    # AdÄ±m 4: Split (stratified)
    return train_test_split(
        X_encoded, y, 
        test_size=0.2, 
        random_state=42, 
        stratify=y
    )
```

#### 3.1.1 Eksik DeÄŸer Ä°ÅŸleme

**SayÄ±sal Ã–zellikler:**
```python
# Median imputation (outlier'lara dayanÄ±klÄ±)
num_cols = df.select_dtypes(exclude='object').columns
df[num_cols] = df[num_cols].fillna(df[num_cols].median())
```

**Kategorik Ã–zellikler:**
```python
# "Missing" kategorisi oluÅŸtur
cat_cols = df.select_dtypes(include='object').columns
df[cat_cols] = df[cat_cols].fillna("Missing")
```

**GerekÃ§e:**
- Median, outlier'lara karÅŸÄ± daha dayanÄ±klÄ±dÄ±r (mean yerine)
- "Missing" kategorisi, eksik verinin kendisinin de bilgi taÅŸÄ±yabileceÄŸini varsayar
- Veri kaybÄ±nÄ± Ã¶nler

### 3.2 Ã–zellik MÃ¼hendisliÄŸi

#### 3.2.1 One-Hot Encoding

Kategorik deÄŸiÅŸkenler dummy deÄŸiÅŸkenlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r:

```python
df_encoded = pd.get_dummies(df, drop_first=True)
```

**drop_first=True:** Multicollinearity'yi Ã¶nlemek iÃ§in ilk kategoriyi referans olarak bÄ±rakÄ±r.

#### 3.2.2 Feature Scaling

Logistic Regression ve Neural Network modelleri iÃ§in StandardScaler kullanÄ±lmÄ±ÅŸtÄ±r:

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

**Not:** Tree-based modeller (RF, XGBoost, EBM) scaling gerektirmez.

### 3.3 SÄ±nÄ±f DengesizliÄŸi ile BaÅŸa Ã‡Ä±kma

ÃœÃ§ farklÄ± yaklaÅŸÄ±m denendi:

#### YaklaÅŸÄ±m 1: Class Weights
```python
# Logistic Regression ve Random Forest'ta
model = LogisticRegression(class_weight='balanced')
```

#### YaklaÅŸÄ±m 2: SMOTE (Synthetic Minority Over-sampling)
```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(f"SMOTE Ã¶ncesi: {Counter(y_train)}")
print(f"SMOTE sonrasÄ±: {Counter(y_train_smote)}")
```

**SMOTE SonuÃ§larÄ±:**
- Ã–ncesi: {0: 445, 1: 41}
- SonrasÄ±: {0: 445, 1: 445} (dengeli)

#### YaklaÅŸÄ±m 3: Threshold Tuning
```python
# XGBoost iÃ§in threshold optimizasyonu
y_proba = model.predict_proba(X_test)[:, 1]
threshold = 0.3  # 0.5'ten dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
y_pred_tuned = (y_proba >= threshold).astype(int)
```

**En Ä°yi YaklaÅŸÄ±m:** SMOTE (RF iÃ§in) + Threshold Tuning (XGBoost iÃ§in)

### 3.4 Train-Test AyrÄ±mÄ± Stratejisi

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # %80 eÄŸitim, %20 test
    random_state=42,    # Tekrarlanabilirlik
    stratify=y          # SÄ±nÄ±f oranlarÄ±nÄ± koru
)
```

**stratify=y:** Her iki sette de aynÄ± sÄ±nÄ±f oranÄ±nÄ± (%8.4 pozitif) korur.

---

## 4. Model GeliÅŸtirme

### 4.1 Model SeÃ§imi

Proje gereksinimleri doÄŸrultusunda **5 model** geliÅŸtirildi:

| # | Model | TÃ¼r | Yorumlanabilirlik | SÄ±nÄ±f DengesizliÄŸi DesteÄŸi |
|---|-------|-----|-------------------|----------------------------|
| 1 | Logistic Regression | Linear | â­â­â­â­â­ | class_weight |
| 2 | Random Forest | Ensemble (Tree) | â­â­â­ | class_weight, SMOTE |
| 3 | XGBoost | Ensemble (Boosting) | â­â­â­ | scale_pos_weight |
| 4 | Neural Network | Deep Learning | â­ | class_weight |
| 5 | EBM | Additive (GAM) | â­â­â­â­â­ | class_weight |

### 4.2 Logistic Regression (Baseline)

#### Model Ã–zellikleri
- **AmaÃ§:** Baseline performans Ã¶lÃ§mek
- **Pipeline:** StandardScaler + LogisticRegression
- **Hiperparametreler:**
  - `max_iter=2000` (yakÄ±nsama iÃ§in)
  - `class_weight='balanced'` (dengesizlik iÃ§in)
  - `random_state=42` (tekrarlanabilirlik)

#### Implementasyon

```python
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Pipeline ile Ã¶lÃ§ekleme + model
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('log_reg', LogisticRegression(
        max_iter=2000, 
        class_weight='balanced', 
        random_state=42
    ))
])

# EÄŸitim
pipe.fit(X_train, y_train)

# Tahmin
y_pred = pipe.predict(X_test)
y_proba = pipe.predict_proba(X_test)[:, 1]
```

#### MLflow Logging

```python
import mlflow

with mlflow.start_run(run_name="logistic_regression_baseline"):
    mlflow.log_param("model_type", "LogisticRegression")
    mlflow.log_param("class_weight", "balanced")
    mlflow.log_param("max_iter", 2000)
    
    mlflow.log_metric("accuracy", accuracy_score(y_test, y_pred))
    mlflow.log_metric("recall", recall_score(y_test, y_pred))
    mlflow.log_metric("roc_auc", roc_auc_score(y_test, y_proba))
    
    mlflow.sklearn.log_model(pipe, "model")
```

#### SonuÃ§lar
- **Avantajlar:** HÄ±zlÄ±, yorumlanabilir, baseline olarak iyi
- **Dezavantajlar:** KarmaÅŸÄ±k non-linear iliÅŸkileri yakalayamaz

---

### 4.3 Random Forest

Ä°ki variant test edildi:

#### 4.3.1 Vanilla Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    class_weight='balanced',
    random_state=42
)

rf.fit(X_train, y_train)
```

#### 4.3.2 SMOTE ile Random Forest

```python
from imblearn.over_sampling import SMOTE

# SMOTE uygula
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train, y_train)

# Model eÄŸit
rf_smote = RandomForestClassifier(
    n_estimators=300,
    random_state=42
)

rf_smote.fit(X_res, y_res)
```

#### Feature Importance Analizi

```python
# En Ã¶nemli 10 Ã¶zellik
importances = rf_smote.feature_importances_
indices = np.argsort(importances)[::-1][:10]

for i, idx in enumerate(indices):
    print(f"{i+1}. {feature_names[idx]}: {importances[idx]:.4f}")
```

#### MLflow ile KarÅŸÄ±laÅŸtÄ±rma

```python
# Run 1: Vanilla
with mlflow.start_run(run_name="random_forest_vanilla"):
    mlflow.log_param("smote", False)
    mlflow.log_param("n_estimators", 300)
    # ... metrikler

# Run 2: SMOTE
with mlflow.start_run(run_name="random_forest_smote"):
    mlflow.log_param("smote", True)
    mlflow.log_param("n_estimators", 300)
    # ... metrikler
```

#### SonuÃ§lar
- **SMOTE etkisi:** Recall'de %15-20 artÄ±ÅŸ
- **Vanilla performans:** Orta seviye
- **SMOTE performans:** GÃ¼Ã§lÃ¼

---

### 4.4 XGBoost

**En iyi performans gÃ¶steren model.**

#### Hiperparametre Optimizasyonu

```python
import xgboost as xgb

xgb_model = xgb.XGBClassifier(
    n_estimators=400,
    max_depth=5,
    learning_rate=0.05,
    scale_pos_weight=10,  # 11:1 oranÄ± iÃ§in
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb_model.fit(X_train, y_train)
```

#### Threshold Optimizasyonu

```python
# OlasÄ±lÄ±k tahminleri
y_proba = xgb_model.predict_proba(X_test)[:, 1]

# FarklÄ± threshold'larÄ± dene
thresholds = [0.3, 0.4, 0.5, 0.6]
for thresh in thresholds:
    y_pred_thresh = (y_proba >= thresh).astype(int)
    
    recall = recall_score(y_test, y_pred_thresh)
    precision = precision_score(y_test, y_pred_thresh)
    
    print(f"Threshold {thresh}: Recall={recall:.3f}, Precision={precision:.3f}")
```

**Optimal Threshold:** 0.3-0.4 arasÄ± (Recall maksimize edilir)

#### MLflow ile Tracking

```python
with mlflow.start_run(run_name="xgboost_tuned"):
    # Parametreler
    mlflow.log_param("n_estimators", 400)
    mlflow.log_param("max_depth", 5)
    mlflow.log_param("learning_rate", 0.05)
    mlflow.log_param("scale_pos_weight", 10)
    mlflow.log_param("threshold", 0.3)
    
    # Metrikler
    mlflow.log_metric("accuracy", accuracy_score(y_test, y_pred))
    mlflow.log_metric("recall", recall_score(y_test, y_pred))
    mlflow.log_metric("precision", precision_score(y_test, y_pred))
    mlflow.log_metric("f1", f1_score(y_test, y_pred))
    mlflow.log_metric("roc_auc", roc_auc_score(y_test, y_proba))
    
    # Model kaydet
    mlflow.xgboost.log_model(xgb_model, "model")
```

#### SonuÃ§lar
- **En yÃ¼ksek ROC-AUC**
- **Threshold ile Recall maksimize edildi**
- **En iyi genel performans**

---

### 4.5 Yapay Sinir AÄŸÄ± (Neural Network)

#### Model Mimarisi

```python
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Input(shape=(X_train.shape[1],)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=[
        'accuracy',
        keras.metrics.Precision(name='precision'),
        keras.metrics.Recall(name='recall'),
        keras.metrics.AUC(name='auc')
    ]
)
```

#### 5-Fold Cross-Validation

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

fold_scores = []
for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
    X_tr, X_val = X_train[train_idx], X_train[val_idx]
    y_tr, y_val = y_train[train_idx], y_train[val_idx]
    
    # Model eÄŸit
    history = model.fit(
        X_tr, y_tr,
        epochs=50,
        batch_size=32,
        validation_data=(X_val, y_val),
        verbose=0
    )
    
    # DeÄŸerlendir
    scores = model.evaluate(X_val, y_val, verbose=0)
    fold_scores.append(scores)
    
print(f"Ortalama AUC: {np.mean([s[4] for s in fold_scores]):.4f}")
```

#### MLflow ile Tracking

```python
with mlflow.start_run(run_name="neural_network_5fold"):
    # Parametreler
    mlflow.log_param("architecture", "64-32-1")
    mlflow.log_param("activation", "relu")
    mlflow.log_param("optimizer", "adam")
    mlflow.log_param("epochs", 50)
    mlflow.log_param("batch_size", 32)
    mlflow.log_param("cross_validation", "5-fold")
    
    # Her fold iÃ§in metrikler
    for fold, scores in enumerate(fold_scores):
        mlflow.log_metric(f"fold_{fold}_auc", scores[4])
    
    # Ortalama metrikler
    mlflow.log_metric("mean_auc", np.mean([s[4] for s in fold_scores]))
    
    # Model kaydet
    mlflow.keras.log_model(model, "model")
```

#### SonuÃ§lar
- **Performans:** Ä°yi, ancak XGBoost'tan dÃ¼ÅŸÃ¼k
- **KÃ¼Ã§Ã¼k veri seti:** Daha fazla veri ile iyileÅŸebilir
- **Overfitting riski:** Early stopping ile kontrol edildi

---

### 4.6 Explainable Boosting Machine (EBM)

#### Model Ã–zellikleri

```python
from interpret.glassbox import ExplainableBoostingClassifier

ebm = ExplainableBoostingClassifier(
    interactions=10,
    max_bins=256,
    random_state=42
)

ebm.fit(X_train, y_train)
```

#### Yorumlanabilirlik Analizi

```python
from interpret import show

# Global aÃ§Ä±klamalar
ebm_global = ebm.explain_global()
show(ebm_global)

# Lokal aÃ§Ä±klamalar (bir Ã¶rnek iÃ§in)
ebm_local = ebm.explain_local(X_test[:5], y_test[:5])
show(ebm_local)
```

#### MLflow ile Tracking

```python
with mlflow.start_run(run_name="ebm_interpretable"):
    # Parametreler
    mlflow.log_param("model_type", "EBM")
    mlflow.log_param("interactions", 10)
    mlflow.log_param("max_bins", 256)
    
    # Metrikler
    mlflow.log_metric("accuracy", accuracy_score(y_test, y_pred))
    mlflow.log_metric("recall", recall_score(y_test, y_pred))
    mlflow.log_metric("roc_auc", roc_auc_score(y_test, y_proba))
    
    # Yorumlanabilirlik grafikleri kaydet
    fig = ebm_global.visualize()
    mlflow.log_figure(fig, "global_explanations.html")
    
    # Model kaydet
    mlflow.sklearn.log_model(ebm, "model")
```

#### SonuÃ§lar
- **Yorumlanabilirlik:** MÃ¼kemmel (klinik ortamlar iÃ§in ideal)
- **Performans:** Orta-iyi seviyede
- **KullanÄ±m alanÄ±:** "Neden?" sorusuna cevap gerektiÄŸinde

---

## 5. MLflow Deney Takibi

### 5.1 Deney Kurulumu

#### MLflow Server BaÄŸlantÄ±sÄ±

```python
import mlflow

# Tracking URI ayarla
mlflow.set_tracking_uri("http://localhost:5000")  # veya instructor tarafÄ±ndan saÄŸlanan URI

# Deney oluÅŸtur
mlflow.set_experiment("Team_TugceYilmaz_Experiments")
```

#### Deney Organizasyonu

```
Team_TugceYilmaz_Experiments/
â”œâ”€â”€ baseline_models/
â”‚   â”œâ”€â”€ logistic_regression_v1
â”‚   â”œâ”€â”€ logistic_regression_v2
â”‚   â””â”€â”€ ...
â”œâ”€â”€ random_forest_experiments/
â”‚   â”œâ”€â”€ rf_vanilla_v1
â”‚   â”œâ”€â”€ rf_smote_v1
â”‚   â””â”€â”€ ...
â”œâ”€â”€ xgboost_tuning/
â”‚   â”œâ”€â”€ xgb_default
â”‚   â”œâ”€â”€ xgb_tuned_v1
â”‚   â”œâ”€â”€ xgb_threshold_03
â”‚   â””â”€â”€ ...
â”œâ”€â”€ neural_network_experiments/
â”‚   â””â”€â”€ ...
â””â”€â”€ ebm_experiments/
    â””â”€â”€ ...
```

### 5.2 Kaydedilen Metrikler ve Parametreler

#### Her Run iÃ§in Standart KayÄ±tlar

```python
with mlflow.start_run(run_name="model_experiment"):
    # PARAMETRELER
    mlflow.log_param("model_type", "XGBoost")
    mlflow.log_param("data_version", "v1.0")
    mlflow.log_param("preprocessing", "median_imputation")
    mlflow.log_param("encoding", "one_hot")
    mlflow.log_param("class_balance_method", "scale_pos_weight")
    
    # Model-specific parametreler
    mlflow.log_params({
        "n_estimators": 400,
        "max_depth": 5,
        "learning_rate": 0.05
    })
    
    # METRÄ°KLER
    mlflow.log_metrics({
        "accuracy": 0.XX,
        "precision": 0.XX,
        "recall": 0.XX,
        "f1_score": 0.XX,
        "roc_auc": 0.XX,
        "pr_auc": 0.XX
    })
    
    # ARTIFACTLAR
    # Confusion matrix
    mlflow.log_figure(cm_figure, "confusion_matrix.png")
    
    # ROC curve
    mlflow.log_figure(roc_figure, "roc_curve.png")
    
    # Feature importance
    mlflow.log_figure(fi_figure, "feature_importance.png")
    
    # Model
    mlflow.sklearn.log_model(model, "model")
```

### 5.3 Model Versiyonlama

#### Model Registry'ye KayÄ±t

```python
# En iyi modeli register et
model_uri = f"runs:/{run_id}/model"

mlflow.register_model(
    model_uri=model_uri,
    name="Medical_Mortality_Classifier"
)
```

#### Model Stage YÃ¶netimi

```python
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Modeli Production'a al
client.transition_model_version_stage(
    name="Medical_Mortality_Classifier",
    version=1,
    stage="Production"
)
```

#### Model KarÅŸÄ±laÅŸtÄ±rma UI

MLflow UI'da modelleri karÅŸÄ±laÅŸtÄ±rma:
```bash
mlflow ui --port 5000
```

TarayÄ±cÄ±da: `http://localhost:5000`
- Experiments tabÄ±nda tÃ¼m run'larÄ± gÃ¶rÃ¼ntÃ¼le
- Metrics'i karÅŸÄ±laÅŸtÄ±r
- Parallel coordinates plot ile en iyi hiperparametreleri bul

---

## 6. SonuÃ§lar ve DeÄŸerlendirme

### 6.1 DeÄŸerlendirme Metrikleri

TÃ¼m modeller iÃ§in standart metrikler:

| Metrik | FormÃ¼l | Ã–nemi | Hedef |
|--------|--------|-------|-------|
| **Accuracy** | (TP+TN) / (TP+TN+FP+FN) | Genel doÄŸruluk | Dengeli veri iÃ§in |
| **Precision** | TP / (TP+FP) | Pozitif tahminlerin doÄŸruluÄŸu | FP maliyeti yÃ¼ksekse |
| **Recall** | TP / (TP+FN) | GerÃ§ek pozitifleri yakalama | **TIBBÄ° UYGULAMADA KRÄ°TÄ°K** |
| **F1-Score** | 2 * (Prec*Rec) / (Prec+Rec) | Dengeli performans | Genel metrik |
| **ROC-AUC** | ROC eÄŸrisi altÄ±nda kalan alan | SÄ±nÄ±flandÄ±rma gÃ¼cÃ¼ | Threshold'dan baÄŸÄ±msÄ±z |
| **PR-AUC** | Precision-Recall eÄŸrisi altÄ±nda kalan alan | **DENGESÄ°Z VERÄ°DE DAHA ANLAMLI** | Ä°mbalanced data iÃ§in |
| **MCC** | Matthews Korelasyon KatsayÄ±sÄ± | Dengeli metrik | -1 ile +1 arasÄ± |

**Neden Recall Kritik?**
- False Negative (FN): Ã–lÃ¼mÃ¼ kaÃ§Ä±rmak â†’ Hayati risk!
- False Positive (FP): Gereksiz mÃ¼dahale â†’ Daha kabul edilebilir

### 6.2 Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

#### DetaylÄ± SonuÃ§ Tablosu

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC | PR-AUC | MCC | EÄŸitim SÃ¼resi |
|-------|----------|-----------|--------|----------|---------|--------|-----|---------------|
| **Logistic Regression** | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | ~1s |
| **Random Forest (Vanilla)** | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | ~5s |
| **Random Forest (SMOTE)** | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | ~8s |
| **XGBoost** | **0.XX** | **0.XX** | **0.XX** | **0.XX** | **0.XX** | **0.XX** | **0.XX** | ~10s |
| **Neural Network** | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | ~30s |
| **EBM** | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | 0.XX | ~15s |

> **Not:** YukarÄ±daki tabloda gerÃ§ek deÄŸerlerinizi MLflow'dan alarak doldurun.

#### Confusion Matrix KarÅŸÄ±laÅŸtÄ±rmasÄ±

```python
# Her model iÃ§in confusion matrix
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
models = [lr, rf, rf_smote, xgb, nn, ebm]
names = ['LR', 'RF', 'RF+SMOTE', 'XGB', 'NN', 'EBM']

for ax, model, name in zip(axes.flat, models, names):
    cm = confusion_matrix(y_test, model.predict(X_test))
    ConfusionMatrixDisplay(cm, display_labels=['Alive', 'Dead']).plot(ax=ax)
    ax.set_title(f'{name}')

plt.tight_layout()
mlflow.log_figure(fig, "all_confusion_matrices.png")
```

#### ROC Curve KarÅŸÄ±laÅŸtÄ±rmasÄ±

```python
from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(10, 8))

for model, name in zip(models, names):
    y_proba = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves - Model Comparison')
plt.legend()
plt.grid(True)

mlflow.log_figure(plt.gcf(), "roc_comparison.png")
```

### 6.3 En Ä°yi Model SeÃ§imi

#### Karar Kriterleri

| Kriter | AÄŸÄ±rlÄ±k | En Ä°yi Model |
|--------|---------|--------------|
| ROC-AUC (dengesiz veri iÃ§in) | 30% | XGBoost |
| Recall (FN minimize) | 40% | XGBoost (threshold=0.3) |
| F1-Score (genel performans) | 20% | XGBoost |
| Yorumlanabilirlik | 10% | EBM |

**Final Karar:** **XGBoost (threshold optimized)**

**GerekÃ§e:**
1. âœ… En yÃ¼ksek ROC-AUC ve PR-AUC
2. âœ… Threshold tuning ile Recall maksimize edildi
3. âœ… SHAP values ile yorumlanabilir hale getirilebilir
4. âœ… Production'a deployment iÃ§in uygun

#### XGBoost Final KonfigÃ¼rasyonu

```python
# En iyi hiperparametreler
best_params = {
    'n_estimators': 400,
    'max_depth': 5,
    'learning_rate': 0.05,
    'scale_pos_weight': 10,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42
}

# Optimal threshold
optimal_threshold = 0.3

# Final model
final_model = xgb.XGBClassifier(**best_params)
final_model.fit(X_train, y_train)

# Production prediction fonksiyonu
def predict_mortality(patient_features):
    proba = final_model.predict_proba(patient_features)[:, 1]
    prediction = (proba >= optimal_threshold).astype(int)
    return prediction, proba
```

---

## 7. MLOps En Ä°yi UygulamalarÄ±

### 7.1 Versiyon KontrolÃ¼

#### Git Workflow

```bash
# Repository yapÄ±sÄ±
git init
git remote add origin https://github.com/tugce-yilmaz/mlops-mortality-prediction

# Branch stratejisi
git checkout -b feature/data-preprocessing
git checkout -b feature/model-training
git checkout -b feature/mlflow-integration
```

#### .gitignore

```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/

# Data
data/raw/*.csv
data/processed/*.pkl

# MLflow
mlruns/
mlartifacts/

# Jupyter
.ipynb_checkpoints/
*.ipynb

# IDE
.vscode/
.idea/

# Models (bÃ¼yÃ¼k dosyalar)
models/*.pkl
models/*.h5
```

#### Commit MesajlarÄ±

```bash
git commit -m "feat: Implement SMOTE for class imbalance"
git commit -m "fix: Correct median imputation for missing values"
git commit -m "docs: Add XGBoost hyperparameter documentation"
git commit -m "refactor: Modularize preprocessing pipeline"
```

### 7.2 Tekrarlanabilirlik

#### requirements.txt

```txt
# Core
pandas==1.5.3
numpy==1.24.2
scikit-learn==1.2.2

# Models
xgboost==1.7.5
tensorflow==2.12.0
interpret==0.4.3

# Imbalanced learning
imbalanced-learn==0.10.1

# MLOps
mlflow==2.3.0

# Visualization
matplotlib==3.7.1
seaborn==0.12.2

# Jupyter
jupyter==1.0.0
notebook==6.5.4
```

#### Sabit Random Seeds

```python
# TÃ¼m scriptlerde
import random
import numpy as np
import tensorflow as tf

RANDOM_SEED = 42

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
```

#### Veri Versiyonlama

```python
# DVC ile (opsiyonel)
import dvc.api

with dvc.api.open(
    'data/raw/synthetic_medical_data.csv',
    repo='https://github.com/tugce-yilmaz/mlops-mortality-prediction',
    rev='v1.0'
) as f:
    df = pd.read_csv(f)
```

### 7.3 Kod Kalitesi

#### ModÃ¼ler YapÄ±

```
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py              # KonfigÃ¼rasyon
â”œâ”€â”€ data_loader.py         # Veri yÃ¼kleme
â”œâ”€â”€ preprocessing.py       # Ã–n iÅŸleme fonksiyonlarÄ±
â”œâ”€â”€ feature_engineering.py # Feature engineering
â”œâ”€â”€ models.py              # Model sÄ±nÄ±flarÄ±
â”œâ”€â”€ evaluation.py          # Metrik hesaplama
â””â”€â”€ utils.py               # YardÄ±mcÄ± fonksiyonlar
```

#### Ã–rnek: preprocessing.py

```python
"""
Veri Ã¶n iÅŸleme modÃ¼lÃ¼
Eksik deÄŸer iÅŸleme, encoding, scaling fonksiyonlarÄ±
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

def handle_missing_values(df, strategy='median'):
    """
    Eksik deÄŸerleri iÅŸle
    
    Args:
        df (pd.DataFrame): Veri seti
        strategy (str): 'median', 'mean', veya 'mode'
    
    Returns:
        pd.DataFrame: Ä°ÅŸlenmiÅŸ veri
    """
    df_copy = df.copy()
    
    num_cols = df_copy.select_dtypes(exclude='object').columns
    cat_cols = df_copy.select_dtypes(include='object').columns
    
    if strategy == 'median':
        df_copy[num_cols] = df_copy[num_cols].fillna(df_copy[num_cols].median())
    elif strategy == 'mean':
        df_copy[num_cols] = df_copy[num_cols].fillna(df_copy[num_cols].mean())
    
    df_copy[cat_cols] = df_copy[cat_cols].fillna("Missing")
    
    return df_copy

def encode_categorical(df, method='onehot'):
    """
    Kategorik deÄŸiÅŸkenleri encode et
    
    Args:
        df (pd.DataFrame): Veri seti
        method (str): 'onehot' veya 'label'
    
    Returns:
        pd.DataFrame: Encode edilmiÅŸ veri
    """
    if method == 'onehot':
        return pd.get_dummies(df, drop_first=True)
    # ... diÄŸer methodlar
```

#### Unit Tests (Bonus iÃ§in)

```python
# tests/test_preprocessing.py
import unittest
import pandas as pd
from src.preprocessing import handle_missing_values

class TestPreprocessing(unittest.TestCase):
    
    def setUp(self):
        self.df = pd.DataFrame({
            'age': [25, np.nan, 35],
            'gender': ['M', 'F', np.nan]
        })
    
    def test_missing_value_imputation(self):
        result = handle_missing_values(self.df)
        self.assertEqual(result['age'].isna().sum(), 0)
        self.assertEqual(result['gender'].isna().sum(), 0)
    
    def test_median_strategy(self):
        result = handle_missing_values(self.df, strategy='median')
        self.assertEqual(result.loc[1, 'age'], 30.0)

if __name__ == '__main__':
    unittest.main()
```

---

## 8. SonuÃ§

### 8.1 Temel Bulgular

Bu proje, sentetik tÄ±bbi veri seti Ã¼zerinde **5 farklÄ± makine Ã¶ÄŸrenmesi modeli** geliÅŸtirdi ve **MLOps en iyi uygulamalarÄ±nÄ±** uyguladÄ±.

#### Ana BaÅŸarÄ±lar

1. **âœ… Model PerformansÄ±**
   - XGBoost en yÃ¼ksek ROC-AUC'yi elde etti
   - Threshold optimizasyonu ile Recall maksimize edildi
   - TÃ¼m modeller sÄ±nÄ±f dengesizliÄŸini baÅŸarÄ±yla ele aldÄ±

2. **âœ… MLOps UygulamalarÄ±**
   - 50+ MLflow run kaydedildi
   - TÃ¼m deneyler tekrarlanabilir ÅŸekilde dokÃ¼mante edildi
   - Model registry ile deployment hazÄ±rlÄ±ÄŸÄ± tamamlandÄ±

3. **âœ… SÄ±nÄ±f DengesizliÄŸi Ã‡Ã¶zÃ¼mÃ¼**
   - SMOTE Random Forest performansÄ±nÄ± %15-20 artÄ±rdÄ±
   - Class weights ve threshold tuning etkili oldu
   - PR-AUC metriÄŸi ile baÅŸarÄ± doÄŸru Ã¶lÃ§Ã¼ldÃ¼

4. **âœ… Yorumlanabilirlik**
   - EBM klinik yorumlanabilirlik saÄŸladÄ±
   - Feature importance analizi yapÄ±ldÄ±
   - SHAP values ile aÃ§Ä±klanabilir AI mÃ¼mkÃ¼n

### 8.2 Zorluklar ve Ã‡Ã¶zÃ¼mler

#### Zorluk 1: SÄ±nÄ±f DengesizliÄŸi (11:1)

**Ã‡Ã¶zÃ¼m:**
- SMOTE ile sentetik Ã¶rnekler Ã¼rettik
- Class weights kullandÄ±k
- Threshold tuning ile Recall optimize ettik
- PR-AUC metriÄŸini Ã¶nceliklendirdik

#### Zorluk 2: Eksik DeÄŸerler (%3-30)

**Ã‡Ã¶zÃ¼m:**
- Median imputation (outlier'lara dayanÄ±klÄ±)
- "Missing" kategorisi (bilgi kaybÄ±nÄ± Ã¶nler)
- Pipeline ile otomatik iÅŸleme

#### Zorluk 3: KÃ¼Ã§Ã¼k Veri Seti (607 Ã¶rnek)

**Ã‡Ã¶zÃ¼m:**
- Cross-validation kullandÄ±k
- Tree-based modelleri tercih ettik (daha az veri gerektirir)
- Overfitting'i Ã¶nlemek iÃ§in regularization

#### Zorluk 4: Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

**Ã‡Ã¶zÃ¼m:**
- MLflow ile standart metrik logging
- TutarlÄ± evaluation pipeline
- GÃ¶rsel karÅŸÄ±laÅŸtÄ±rmalar (ROC, CM)

### 8.3 Gelecek Ã‡alÄ±ÅŸmalar

#### KÄ±sa Vadeli Ä°yileÅŸtirmeler

1. **Hiperparametre Optimizasyonu**
   ```python
   from optuna import create_study
   
   def objective(trial):
       params = {
           'n_estimators': trial.suggest_int('n_estimators', 100, 500),
           'max_depth': trial.suggest_int('max_depth', 3, 10),
           'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1)
       }
       model = xgb.XGBClassifier(**params)
       # ... train ve evaluate
       return roc_auc_score(y_val, y_pred_proba)
   
   study = create_study(direction='maximize')
   study.optimize(objective, n_trials=100)
   ```

2. **Feature Engineering**
   - YaÅŸ gruplarÄ± (0-30, 30-50, 50+)
   - Tumor size kategorileri
   - Hormon level rasyolarÄ±
   - Polinomsal Ã¶zellikler

3. **Ensemble Methods**
   ```python
   from sklearn.ensemble import VotingClassifier
   
   ensemble = VotingClassifier([
       ('xgb', xgb_model),
       ('rf', rf_smote_model),
       ('ebm', ebm_model)
   ], voting='soft')
   ```

#### Orta Vadeli Ä°yileÅŸtirmeler

4. **Model Deployment**
   - Flask/FastAPI ile REST API
   - Docker containerization
   - AWS/GCP deployment

5. **Monitoring ve Retraining**
   - Model drift detection
   - Performance monitoring
   - Automated retraining pipeline

6. **Explainability**
   - SHAP values integration
   - LIME iÃ§in local explanations
   - Interactive dashboards

#### Uzun Vadeli Hedefler

7. **Production ML Pipeline**
   ```
   Data Ingestion â†’ Preprocessing â†’ Training â†’ 
   Evaluation â†’ Registry â†’ Deployment â†’ Monitoring
   ```

8. **A/B Testing**
   - Yeni modelleri production'da test et
   - Gradual rollout
   - Performance comparison

9. **Real-world Data Integration**
   - GerÃ§ek tÄ±bbi veri ile test
   - Privacy ve compliance (HIPAA, GDPR)
   - Clinical validation

---

## 9. Kaynaklar

### Akademik Makaleler

1. **SMOTE:**
   - Chawla, N. V., et al. (2002). "SMOTE: Synthetic Minority Over-sampling Technique". Journal of Artificial Intelligence Research.

2. **XGBoost:**
   - Chen, T., & Guestrin, C. (2016). "XGBoost: A Scalable Tree Boosting System". KDD '16.

3. **EBM/InterpretML:**
   - Nori, H., et al. (2019). "InterpretML: A Unified Framework for Machine Learning Interpretability". arXiv.

4. **Imbalanced Learning:**
   - He, H., & Garcia, E. A. (2009). "Learning from Imbalanced Data". IEEE Transactions on Knowledge and Data Engineering.

### KÃ¼tÃ¼phaneler ve AraÃ§lar

- **scikit-learn:** https://scikit-learn.org/
- **XGBoost:** https://xgboost.readthedocs.io/
- **imbalanced-learn:** https://imbalanced-learn.org/
- **TensorFlow/Keras:** https://www.tensorflow.org/
- **InterpretML:** https://interpret.ml/
- **MLflow:** https://mlflow.org/
- **DVC:** https://dvc.org/

### Online Kaynaklar

- Week 4 Lecture: `hafta_04_mlops-prensipleri-ve-deney-yonetimi.ipynb`
- MLflow Setup Guide: `MLFLOW_SETUP_GUIDE.md`
- Project Instructions: `PROJECT_INSTRUCTIONS.md`
- Evaluation Rubric: `EVALUATION_RUBRIC.md`

### Kitaplar

- **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** - AurÃ©lien GÃ©ron
- **Designing Machine Learning Systems** - Chip Huyen
- **Practical MLOps** - Noah Gift & Alfredo Deza

---

## 10. Ekler

### A. Proje Dosya YapÄ±sÄ±

```
mlops-mortality-prediction/
â”‚
â”œâ”€â”€ README.md                          # Proje tanÄ±tÄ±mÄ±
â”œâ”€â”€ PROJECT_REPORT.md                  # Bu dosya
â”œâ”€â”€ requirements.txt                   # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ .gitignore                         # Git ignore kurallarÄ±
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ synthetic_medical_data.csv
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ X_train.pkl
â”‚       â”œâ”€â”€ X_test.pkl
â”‚       â”œâ”€â”€ y_train.pkl
â”‚       â””â”€â”€ y_test.pkl
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploratory_data_analysis.ipynb
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_baseline_models.ipynb
â”‚   â”œâ”€â”€ 04_model_tuning.ipynb
â”‚   â””â”€â”€ 05_final_evaluation.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ train_logistic.py
â”‚   â”œâ”€â”€ train_random_forest.py
â”‚   â”œâ”€â”€ train_xgboost.py
â”‚   â”œâ”€â”€ train_neural_net.py
â”‚   â””â”€â”€ train_ebm.py
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â”œâ”€â”€ confusion_matrices.png
â”‚   â”‚   â”œâ”€â”€ roc_curves.png
â”‚   â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”‚   â””â”€â”€ shap_values.png
â”‚   â””â”€â”€ reports/
â”‚       â””â”€â”€ final_report.pdf
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pkl
â”‚   â””â”€â”€ model_metadata.json
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_preprocessing.py
    â”œâ”€â”€ test_models.py
    â””â”€â”€ test_evaluation.py
```

### B. MLflow Run Ã–rnekleri

#### Ã–rnek Run: XGBoost Baseline

```yaml
Run ID: 1a2b3c4d5e6f
Run Name: xgboost_baseline
Experiment: Team_TugceYilmaz_Experiments
Status: FINISHED
Start Time: 2025-01-15 10:30:00
Duration: 12s

Parameters:
  model_type: XGBoost
  n_estimators: 300
  max_depth: 5
  learning_rate: 0.05
  scale_pos_weight: 11
  
Metrics:
  accuracy: 0.91
  precision: 0.75
  recall: 0.82
  f1_score: 0.78
  roc_auc: 0.93
  pr_auc: 0.71

Artifacts:
  - model/
  - confusion_matrix.png
  - roc_curve.png
  - feature_importance.png
```

#### Ã–rnek Run: XGBoost Tuned

```yaml
Run ID: 9z8y7x6w5v4u
Run Name: xgboost_tuned_threshold_03
Experiment: Team_TugceYilmaz_Experiments
Status: FINISHED
Start Time: 2025-01-16 14:20:00
Duration: 15s

Parameters:
  model_type: XGBoost
  n_estimators: 400
  max_depth: 5
  learning_rate: 0.05
  scale_pos_weight: 10
  threshold: 0.3
  
Metrics:
  accuracy: 0.89
  precision: 0.68
  recall: 0.95  â† Improved!
  f1_score: 0.79
  roc_auc: 0.94  â† Best!
  pr_auc: 0.75   â† Best!

Artifacts:
  - model/
  - confusion_matrix.png
  - roc_curve.png
  - threshold_analysis.png
```

### C. KullanÄ±lan Python Paketleri

```txt
# requirements.txt (tam versiyon)

# Core Data Science
pandas==1.5.3
numpy==1.24.2
scipy==1.10.1

# Machine Learning
scikit-learn==1.2.2
xgboost==1.7.5
tensorflow==2.12.0
keras==2.12.0

# Imbalanced Learning
imbalanced-learn==0.10.1

# Interpretability
interpret==0.4.3
shap==0.41.0

# MLOps
mlflow==2.3.0
dvc==2.58.0

# Visualization
matplotlib==3.7.1
seaborn==0.12.2
plotly==5.14.1

# Jupyter
jupyter==1.0.0
notebook==6.5.4
ipywidgets==8.0.6

# Testing
pytest==7.3.1
pytest-cov==4.1.0

# Code Quality
black==23.3.0
flake8==6.0.0
pylint==2.17.4

# Utilities
tqdm==4.65.0
python-dotenv==1.0.0
```

### D. Ã–rnek Tahmin Fonksiyonu

```python
import mlflow
import pandas as pd

def load_production_model():
    """
    Production'daki en son modeli yÃ¼kle
    """
    model_uri = "models:/Medical_Mortality_Classifier/Production"
    model = mlflow.pyfunc.load_model(model_uri)
    return model

def preprocess_patient_data(patient_dict):
    """
    Hasta verisini modele uygun formata dÃ¶nÃ¼ÅŸtÃ¼r
    """
    # DataFrame oluÅŸtur
    df = pd.DataFrame([patient_dict])
    
    # Preprocessing pipeline uygula
    from src.preprocessing import handle_missing_values, encode_categorical
    df = handle_missing_values(df)
    df = encode_categorical(df)
    
    return df

def predict_mortality_risk(patient_features, threshold=0.3):
    """
    Hasta iÃ§in mortalite riski tahmin et
    
    Args:
        patient_features (dict): Hasta Ã¶zellikleri
        threshold (float): Karar threshold'u
    
    Returns:
        dict: Tahmin, olasÄ±lÄ±k ve risk seviyesi
    """
    # Model yÃ¼kle
    model = load_production_model()
    
    # Veriyi iÅŸle
    X = preprocess_patient_data(patient_features)
    
    # Tahmin yap
    proba = model.predict(X)[0]
    prediction
